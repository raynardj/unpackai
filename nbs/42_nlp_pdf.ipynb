{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Utils: extending current functionalities for PDF\n",
    "\n",
    "Add dependency to `pdfminer.six`: use `pip install unpackai[PDF]` to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "from unpackai import utils\n",
    "from unpackai import nlp\n",
    "\n",
    "PathStr = Union[Path, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TextualPDF(nlp.Textual):\n",
    "    \"\"\"Extend Textual for PDF\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_url_pdf(\n",
    "        cls, url: str, password: str = \"\", page_numbers: List[int] = None, cleanup=True\n",
    "    ):\n",
    "        \"\"\"Create a Textual object from a PDF URL with specific options\n",
    "\n",
    "        Args:\n",
    "            url: url of PDF\n",
    "            password: password, if the PDF is protected\n",
    "            page_numbers: list of pages to extract (first page = 0)\n",
    "            cleanup: remove messy characters and line returns (default=True)\n",
    "        \"\"\"\n",
    "        return cls.from_path_pdf(\n",
    "            utils.download(url),\n",
    "            password=password,\n",
    "            page_numbers=page_numbers,\n",
    "            cleanup=cleanup,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_path_pdf(\n",
    "        cls,\n",
    "        pdf_file: PathStr,\n",
    "        password: str = \"\",\n",
    "        page_numbers: List[int] = None,\n",
    "        cleanup=True,\n",
    "    ):\n",
    "        \"\"\"Create a Textual object from a PDF\n",
    "\n",
    "        Args:\n",
    "            pdf_file: path of PDF\n",
    "            password: password, if the PDF is protected\n",
    "            page_numbers: list of pages to extract (first page = 0)\n",
    "            cleanup: remove messy characters and line returns (default=True)\n",
    "        \"\"\"\n",
    "        txt = extract_text(pdf_file, password=password, page_numbers=page_numbers)\n",
    "        if cleanup:\n",
    "            txt = re.sub(r\"[\\r\\n]{2,}\", \"<line_break>\", txt)\n",
    "            txt = re.sub(r\"- *[\\n\\r]\", \"\", txt)\n",
    "            txt = txt.replace(\"\\n\", \" \").replace(\"<line_break>\", \"\\n\\n\")\n",
    "\n",
    "        return cls(txt, Path(pdf_file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: PathStr):\n",
    "        \"\"\"Create a Textual object from a path, including PDF\"\"\"\n",
    "        path = Path(path)\n",
    "        if path.suffix.lower() == \".pdf\":\n",
    "            return cls.from_path_pdf(path)\n",
    "        else:\n",
    "            return super().from_path(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it?\n",
    "\n",
    "```python\n",
    "\n",
    "from unpackai.pdf import TextualPDF\n",
    "\n",
    "textual = TextualPDF.from_url(\"http://islamicblessings.com/upload/A-Thousand-And-One-Nights-1.pdf\")\n",
    "# OR...\n",
    "textual = TextualPDF.from_path(\"C:/my_doc.pdf\")\n",
    "# OR ... if there is a password or you want to extract specific pages...\n",
    "textual = TextualPDF.from_url_pdf(\"https://my_company.com/my_protected_doc.pdf\", password=\"P@ssW0rd\")\n",
    "textual = TextualPDF.from_path_pdf(\"C:/my_doc.pdf\", page_numbers=range(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text (1514113 chars), textual(),\n",
       "    train_path, val_path = textual.create_train_val()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TextualPDF.from_url(\"http://islamicblessings.com/upload/A-Thousand-And-One-Nights-1.pdf\")\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# To be able to run the tests in the Notebook\n",
    "from pathlib import Path\n",
    "import ipytest\n",
    "import sys\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "root_dir = Path(\"..\").resolve()\n",
    "sys.path.append(str(root_dir / \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "# For Test Cases (might have duplicate import because it will be in a dedicated file)\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytest\n",
    "from test_common.utils_4_tests import DATA_DIR, compare_strings\n",
    "from test_utils import GITHUB_TEST_DATA_URL, check_connection_github\n",
    "from unpackai import nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Expected [string #1]\n",
      "+++ Obtained [string #2]\n",
      "@@ -1,6 +1,7 @@\n",
      " \n",
      " Hello, my name is\n",
      "-Jeff\n",
      "+John\n",
      " \n",
      " How are you\n",
      "+today\n",
      " ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "## --- Checking the compare function\n",
    "\n",
    "exp_lines = \"\"\"\n",
    "Hello, my name is\n",
    "John\n",
    "\n",
    "How are you\n",
    "today\n",
    "?\n",
    "\"\"\"\n",
    "\n",
    "obt_lines = \"\"\"\n",
    "Hello, my name is\n",
    "Jeff\n",
    "\n",
    "How are you\n",
    "?\n",
    "\"\"\"\n",
    "\n",
    "print(compare_strings(obt_lines, exp_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "GITHUB_TEST_PDF = f\"{GITHUB_TEST_DATA_URL}/Deep%20learning.pdf\"\n",
    "LOCAL_TEST_PDF = DATA_DIR / \"Deep learning.pdf\"\n",
    "LOCAL_TEST_TXT = DATA_DIR / \"Deep learning.txt\"\n",
    "\n",
    "LOCAL_TEST_PDF_SMALL = DATA_DIR / \"to_download.pdf\"\n",
    "LOCAL_TEST_TXT_SMALL = DATA_DIR / \"to_download.txt\"\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def local_textual():\n",
    "    return TextualPDF.from_path(LOCAL_TEST_PDF)\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_pdf_as_txt():\n",
    "    return Path(LOCAL_TEST_TXT).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def cleanup_spaces(text: str) -> str:\n",
    "    return re.sub(r\"[\\r\\n\\t\\s]+\", \" \", text, flags=re.S)\n",
    "\n",
    "\n",
    "class TestTextualPDF:\n",
    "    def test_from_path(self, local_textual):\n",
    "        \"\"\"Test extract Textual of PDF from local file\"\"\"\n",
    "        t = local_textual\n",
    "        assert \"Deep learning\" in t.text, f\"Text parsed:\\n{t.text}\"\n",
    "\n",
    "    def test_from_path_not_pdf(self):\n",
    "        \"\"\"Test extract Textual of non-PDF file\"\"\"\n",
    "        textual = TextualPDF.from_path(LOCAL_TEST_TXT)\n",
    "        assert textual.text == nlp.Textual.from_path(LOCAL_TEST_TXT).text\n",
    "\n",
    "    def test_from_path_pdf(self, local_textual):\n",
    "        \"\"\"Test extract Textual of PDF from local path using from_path_pdf\"\"\"\n",
    "        textual = TextualPDF.from_path_pdf(LOCAL_TEST_PDF)\n",
    "        assert textual.text == local_textual.text\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"pdf, txt, encoding\",\n",
    "        [\n",
    "            (LOCAL_TEST_PDF_SMALL, LOCAL_TEST_TXT_SMALL, None),\n",
    "            (LOCAL_TEST_PDF, LOCAL_TEST_TXT, \"utf-8\"),\n",
    "        ],\n",
    "        ids=[\"small\", \"utf-8\"],\n",
    "    )\n",
    "    def test_from_path_pdf_no_cleanup(self, pdf: Path, txt: Path, encoding):\n",
    "        \"\"\"Test extract Textual from PDF without cleanup\"\"\"\n",
    "        content = cleanup_spaces(txt.read_text(encoding=encoding))\n",
    "\n",
    "        textual = TextualPDF.from_path_pdf(pdf, cleanup=False)\n",
    "        textual_txt = cleanup_spaces(textual.text)\n",
    "        assert textual_txt == content, compare_strings(content, textual_txt)\n",
    "\n",
    "    @pytest.mark.parametrize(\"pages\", [[0], [0, 1], [0, 2], range(1, 4)])\n",
    "    def test_from_path_pdf_pages(self, pages):\n",
    "        \"\"\"Test extract Textual from PDF with specific pages\"\"\"\n",
    "        text = TextualPDF.from_path_pdf(LOCAL_TEST_PDF, page_numbers=pages).text\n",
    "        for p in pages:\n",
    "            assert f\"Page {p+1}\" in text, f\"Page {p+1} not found in Textual: {text}\"\n",
    "        pages_extracted = re.findall(r\"Page (\\d+)\", text)\n",
    "        extra_pages = set(pages_extracted) - set(str(p + 1) for p in pages)\n",
    "        assert extra_pages == set(), f\"Extra pages extracted: {extra_pages}\"\n",
    "\n",
    "    @pytest.mark.github\n",
    "    def test_from_url(self, check_connection_github, local_textual):\n",
    "        \"\"\"Test extract Textual of PDF from URL\"\"\"\n",
    "        textual = TextualPDF.from_url(GITHUB_TEST_PDF)\n",
    "        assert textual.text == local_textual.text, f\"URL text: {textual.text}\"\n",
    "\n",
    "    @pytest.mark.github\n",
    "    def test_from_url_pdf(self, check_connection_github, local_textual):\n",
    "        \"\"\"Test extract Textual of PDF from URL using from_url_pdf\"\"\"\n",
    "        textual = TextualPDF.from_url_pdf(GITHUB_TEST_PDF)\n",
    "        assert textual.text == local_textual.text, f\"URL text: {textual.text}\"\n",
    "\n",
    "    # TODO: Add Tests with password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                    [100%]\u001b[0m##vso[results.publish type=JUnit;runTitle='Pytest results';]e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml\n",
      "##vso[task.logissue type=warning;]Coverage XML was not created, skipping upload.\n",
      "\n",
      "------------ generated xml file: e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml -------------\n",
      "\u001b[32m\u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 3.73s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
