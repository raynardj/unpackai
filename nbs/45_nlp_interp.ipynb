{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP interpretations\n",
    "> NLL interpretretation tool sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from unpackai.interp.latent import InterpEmbeddings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret huggingface tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class InterpEmbeddingsTokenizer(InterpEmbeddings):\n",
    "    def __init__(self,\n",
    "                 embedding_matrix,\n",
    "                 tokenizer):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        tokenizer: a huggingface tokenizer\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            embedding_matrix,\n",
    "            dict((v, k) for k, v in tokenizer.vocab.items()))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        word: str,\n",
    "        filter_special_token: bool = True,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and\n",
    "            tokenizer's encode/ decode\n",
    "        \"\"\"\n",
    "        token_ids = self.tokenizer.encode(word)\n",
    "        if filter_special_token:\n",
    "            token_ids = list(t for t in token_ids if t > 110)\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity}).head(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "PRETRAINED = \"albert-base-v2\"\n",
    "\n",
    "model = AutoModel.from_pretrained(PRETRAINED)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[CLS]',\n",
       " 'eos_token': '[SEP]',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertEmbeddings(\n",
       "  (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 128)\n",
       "  (token_type_embeddings): Embedding(2, 128)\n",
       "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = model.embeddings.word_embeddings.weight.data.numpy()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddingsTokenizer(\n",
    "    embedding_matrix,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.search(\"wife\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
