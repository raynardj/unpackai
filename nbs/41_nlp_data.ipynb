{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data\n",
    "\n",
    "> Handling NLP data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual data\n",
    "\n",
    "> Raw & pure textual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from forgebox.html import DOM\n",
    "from ipywidgets import interact, interact_manual, FileUpload\n",
    "from unpackai import utils\n",
    "\n",
    "PathStr = Union[Path, str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Textual` can help you\n",
    "\n",
    "### Obtaining data\n",
    "\n",
    "We can extract data from following sources:\n",
    "\n",
    "-   a URL (http or https)\n",
    "-   a path to a textual file (_.txt_, not _.doc_/_.docx_)\n",
    "-   an interactive interface in the notebook, asking to upload the file\n",
    "\n",
    "### Prepare model training\n",
    "\n",
    "We can prepare model training in the following way:\n",
    "\n",
    "-   Pytorch datasets, data collector for dataloader\n",
    "-   Create a hugging face trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Textual:\n",
    "    \"\"\"\n",
    "    Obtain and manage textual data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, path: Optional[Path] = None):\n",
    "        self.text = re.sub(r\"[\\r]?\\n\", \" \", text)\n",
    "        self.path = path\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"\"\"Text ({len(self.text)} chars), textual(),\n",
    "    train_path, val_path = textual.create_train_val()\"\"\"\n",
    "\n",
    "    def __call__(self, page_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Previewing the first 200(or less) pages\n",
    "        - page_size: character number each page\n",
    "        \"\"\"\n",
    "        logging.info(f\"Previewing the first 200 pages\")\n",
    "\n",
    "        @interact\n",
    "        def show_text(page=(0, min(len(self.text)//page_size-1, 200), 1)):\n",
    "            display(self.text[page*page_size: (page+1)*page_size])\n",
    "\n",
    "    @classmethod\n",
    "    def from_url(cls, url: str) -> 'Textual':\n",
    "        \"\"\"Create a Textual object from a URL\"\"\"\n",
    "        return cls.from_path(utils.download(url))\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: PathStr) -> 'Textual':\n",
    "        \"\"\"\n",
    "        Load a textual object from system path\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            raise FileExistsError(f\"Cannot find {path}\")\n",
    "        return cls(path.read_text(encoding=\"utf-8\"), path=path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_upload(\n",
    "        cls,\n",
    "        path: Path = Path(\"./uploaded_file.txt\")\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load textual with interactive upload button\n",
    "        \"\"\"\n",
    "        DOM(\"ðŸ—ƒPlease upload a text file ended in .txt\", \"h4\")()\n",
    "        my_manual = interact_manual.options(manual_name=\"Upload\")\n",
    "\n",
    "        @my_manual\n",
    "        def create_upload(btn_upload=FileUpload(description=\"Choose File\")):\n",
    "            text = list(btn_upload.values())[-1]['content'].decode()\n",
    "            with open(path, \"w\") as f:\n",
    "                f.write(text)\n",
    "            return path\n",
    "\n",
    "        def uploaded():\n",
    "            result = create_upload.widget.result\n",
    "            if result is None:\n",
    "                raise FileExistsError(\n",
    "                    \"You have to upload the txt file first\")\n",
    "            return cls.from_path(result)\n",
    "        return uploaded\n",
    "\n",
    "    def create_train_val(\n",
    "            self,\n",
    "            valid_ratio=.2,\n",
    "            train_path=\"./train_text.txt\",\n",
    "            val_path=\"./val_text.txt\"):\n",
    "        \"\"\"\n",
    "        create 2 files:\n",
    "        - ./train_text.txt\n",
    "        - ./val_text.txt\n",
    "        \"\"\"\n",
    "        split = int(len(self.text)*(valid_ratio))\n",
    "        with open(train_path, \"w\") as f:\n",
    "            f.write(self.text[split:])\n",
    "        with open(val_path, \"w\") as f:\n",
    "            f.write(self.text[:split])\n",
    "        return train_path, val_path\n",
    "\n",
    "    def show_batch(self, tokenizer, bs:int = 4):\n",
    "        from torch.utils.data.dataloader import DataLoader\n",
    "        bunch = self.create_datasets(tokenizer)\n",
    "        return next(iter(DataLoader(\n",
    "            bunch['train_dataset'],\n",
    "            batch_size=bs,\n",
    "            collate_fn = bunch[\"data_collator\"],\n",
    "            )))\n",
    "\n",
    "    def create_datasets(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        valid_ratio: float = .2,\n",
    "        train_path: str = \"./train_text.txt\",\n",
    "        val_path: str = \"./val_text.txt\",\n",
    "        block_size: int = 128,\n",
    "        mlm: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create pytorch datasets and collating functions\n",
    "        - tokenizer: a huggingface tokenizer\n",
    "        - valid ratio: portion of the valid data,\n",
    "            compare to the entire dataset\n",
    "        - train_path: a path saving train text file\n",
    "        - val_path: a path saving valid text file\n",
    "        - block_size: max possible length of the sequence\n",
    "        - mlm, return a masked language modeling collating\n",
    "            default False\n",
    "        \"\"\"\n",
    "        # split dataset\n",
    "        train_path, val_path = self.create_train_val(\n",
    "            valid_ratio=valid_ratio,\n",
    "            train_path=train_path,\n",
    "            val_path=val_path,\n",
    "        )\n",
    "        \n",
    "        from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=train_path,\n",
    "            block_size=block_size)\n",
    "\n",
    "        test_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=val_path,\n",
    "            block_size=block_size)\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=mlm,\n",
    "        )\n",
    "        return dict(\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "    \n",
    "    def get_hf_trainer(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        arguments = None,\n",
    "        valid_ratio: float = .2,\n",
    "        train_path: str = \"./train_text.txt\",\n",
    "        val_path: str = \"./val_text.txt\",\n",
    "        block_size: int = 128,\n",
    "        mlm: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a hugging face trainer\n",
    "        \"\"\"\n",
    "        from transformers import TrainingArguments, Trainer\n",
    "        if arguments is None:\n",
    "            arguments = TrainingArguments(\n",
    "                output_dir=\"./write_style\",\n",
    "                overwrite_output_dir=True,  num_train_epochs=3,\n",
    "                eval_steps = 400, save_steps=800, warmup_steps=600,\n",
    "                per_device_train_batch_size=24,\n",
    "                per_device_eval_batch_size=64,\n",
    "            )\n",
    "        trainer = Trainer(\n",
    "            model=model, args=arguments,\n",
    "            **self.create_datasets(tokenizer,\n",
    "                valid_ratio=valid_ratio,\n",
    "                train_path=train_path,\n",
    "                val_path=val_path,\n",
    "                block_size=block_size,\n",
    "                mlm=mlm)\n",
    "        )\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tools.nbdev_tools import show_doc_enhanced as show_doc\n",
    "show_doc(Textual, show_methods=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create interactive uploading widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = Textual.from_upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual = uploaded()\n",
    "textual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview textual data by page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HFTextBlock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TransformBlock`, designed for huggingface tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def HFTextBlock(tokenizer, **tk_kwargs):\n",
    "    \"\"\"\n",
    "    create Huggingface specialized fastai Block\n",
    "    tokenizer: PreTrainedTokenizer from huggingface\n",
    "    tk_kwargs: keyward arguments for tokenizer's ```__call__```\n",
    "    \"\"\"\n",
    "    from fastai.data.block import TransformBlock\n",
    "    import torch\n",
    "\n",
    "    def text_2_tensor_collate(data):\n",
    "        \"\"\"\n",
    "        During the usual collation\n",
    "        Use tokenizer to encode the text by batch\n",
    "        \"\"\"\n",
    "        cols = list(zip(*data))\n",
    "        result = []\n",
    "        for col in cols:\n",
    "            if type(col[0]) == str:\n",
    "                result.append(\n",
    "                    tokenizer(list(col), return_tensors=\"pt\", **tk_kwargs)[\"input_ids\"]\n",
    "                )\n",
    "            else:\n",
    "                result.append(torch.stack(list(col), 0))\n",
    "        return tuple(result)\n",
    "\n",
    "    def get_hf_text_block():\n",
    "        return TransformBlock(\n",
    "            type_tfms=str, dls_kwargs={\"create_batch\": text_2_tensor_collate}\n",
    "        )\n",
    "\n",
    "    return get_hf_text_block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# To be able to run the tests in the Notebook\n",
    "from pathlib import Path\n",
    "import ipytest\n",
    "import sys\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "root_dir = Path(\"..\").resolve()\n",
    "sys.path.append(str(root_dir / \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "# For Test Cases (might have duplicate import because it will be in a dedicated file)\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytest\n",
    "from test_common.utils_4_tests import DATA_DIR\n",
    "from test_utils import GITHUB_TEST_DATA_URL, check_connection_github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "\n",
    "LOCAL_TEST_TXT = DATA_DIR / \"to_download.txt\"\n",
    "GITHUB_TEST_TXT = f\"{GITHUB_TEST_DATA_URL}/to_download.txt\"\n",
    "\n",
    "LOCAL_TEST_TXT_UTF8 = DATA_DIR / \"Deep learning.txt\"\n",
    "GITHUB_TEST_TXT_UTF8 = f\"{GITHUB_TEST_DATA_URL}/Deep%20learning.txt\"\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def test_txt_content():\n",
    "    return LOCAL_TEST_TXT.read_text()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def local_textual(test_txt_content):\n",
    "    return Textual(test_txt_content)\n",
    "\n",
    "\n",
    "class Test_Textual:\n",
    "    def test_init(self, local_textual, test_txt_content):\n",
    "        \"\"\"Test initialization of Textual from text\"\"\"\n",
    "        expected_txt = test_txt_content.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        assert local_textual.text == expected_txt\n",
    "\n",
    "    def test_init_encoding(self):\n",
    "        \"\"\"Test initialization of Textual from text\"\"\"\n",
    "        content = LOCAL_TEST_TXT_UTF8.read_text(encoding=\"utf-8\")\n",
    "        textual = Textual(content)\n",
    "        assert textual.text == content.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    def test_from_path(self, local_textual):\n",
    "        \"\"\"Test create Textual from path (existing)\"\"\"\n",
    "        textual = Textual.from_path(LOCAL_TEST_TXT)\n",
    "        assert textual.text == local_textual.text\n",
    "\n",
    "    def test_from_path_error(self):\n",
    "        \"\"\"Test extract Textual of file that does not exist\"\"\"\n",
    "        with pytest.raises(FileExistsError):\n",
    "            textual = Textual.from_path(\"does_not_exist.txt\")\n",
    "\n",
    "    @pytest.mark.github\n",
    "    def test_from_url(self, check_connection_github, local_textual):\n",
    "        \"\"\"Test extract Textual from URL\"\"\"\n",
    "        textual = Textual.from_url(GITHUB_TEST_TXT)\n",
    "        assert textual.text == local_textual.text, f\"URL text: {textual.text}\"\n",
    "\n",
    "    @pytest.mark.github\n",
    "    def test_from_url_non_ascii(self, check_connection_github):\n",
    "        \"\"\"Test extract Textual from URL with non-ascii characters\"\"\"\n",
    "        textual = Textual.from_url(GITHUB_TEST_TXT_UTF8)\n",
    "        content = LOCAL_TEST_TXT_UTF8.read_text(encoding=\"utf-8\")\n",
    "        content = content.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        assert textual.text == content, f\"URL text: {textual.text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "ipytest.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
