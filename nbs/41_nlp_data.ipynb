{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data\n",
    "> Handling NLP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual data\n",
    "> Raw & pure textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "from ipywidgets import interact, interact_manual, FileUpload\n",
    "from pathlib import Path\n",
    "from forgebox.html import DOM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textual can help you\n",
    "\n",
    "### Obaining data\n",
    "From following sources\n",
    "* http/ https url\n",
    "* a system path lead to textual file(not doc please)\n",
    "* create an interactive intreface in notebook, asking you to upload the file\n",
    "\n",
    "### Prepare model training\n",
    "With following prividings\n",
    "* Pytorch datasets, data collator for dataloader\n",
    "* Create a hugging face trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Textual:\n",
    "    \"\"\"\n",
    "    Obtain and manage textual data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"\"\"Text ({len(self.text)} chars), textual(),\n",
    "    train_path, val_path = textual.create_train_val()\"\"\"\n",
    "\n",
    "    def __call__(self, page_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Previewing the first 200(or less) pages\n",
    "        - page_size: character number each page\n",
    "        \"\"\"\n",
    "        logging.info(f\"Previewing the first 200 pages\")\n",
    "\n",
    "        @interact\n",
    "        def show_text(page=(0, min(len(self.text)//page_size-1, 200), 1)):\n",
    "            display(self.text[page*page_size: (page+1)*page_size])\n",
    "\n",
    "    @classmethod\n",
    "    def from_url(cls, url: str):\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 200:\n",
    "            res.encoding = 'utf-8'\n",
    "            headers = res.headers\n",
    "            if \"html\" in str(headers).lower():\n",
    "                text = BeautifulSoup(res.text).text\n",
    "            else:\n",
    "                text = res.text\n",
    "            obj = cls(text)\n",
    "            obj.path = \"./text_data.txt\"\n",
    "            with open(obj.path, \"w\") as f:\n",
    "                f.write(obj.text)\n",
    "            return obj\n",
    "        else:\n",
    "            raise ConnectionError(f\"Error downloading: {url}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: Path):\n",
    "        \"\"\"\n",
    "        Load a textual object from system path\n",
    "        \"\"\"\n",
    "        if Path(path).exists() == False:\n",
    "            raise FileExistsError(f\"Can not find {path}\")\n",
    "        with open(path, ) as f:\n",
    "            obj = cls(f.read())\n",
    "            obj.path = path\n",
    "            return obj\n",
    "\n",
    "    @classmethod\n",
    "    def from_upload(\n",
    "        cls,\n",
    "        path: Path = Path(\"./uploaded_file.txt\")\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load textual with interactive upload button\n",
    "        \"\"\"\n",
    "        DOM(\"ðŸ—ƒ Please upload a text file ended in .txt\", \"h4\")()\n",
    "        my_manual = interact_manual.options(manual_name=\"Upload\")\n",
    "\n",
    "        @my_manual\n",
    "        def create_upload(btn_upload=FileUpload(description=\"Choose File\")):\n",
    "            text = list(btn_upload.values())[-1]['content'].decode()\n",
    "            with open(path, \"w\") as f:\n",
    "                f.write(text)\n",
    "            return path\n",
    "\n",
    "        def uploaded():\n",
    "            result = create_upload.widget.result\n",
    "            if result is None:\n",
    "                raise FileExistsError(\n",
    "                    \"You have to upload the txt file first\")\n",
    "            return cls.from_path(result)\n",
    "        return uploaded\n",
    "\n",
    "    def create_train_val(\n",
    "            self,\n",
    "            valid_ratio=.2,\n",
    "            train_path=\"./train_text.txt\",\n",
    "            val_path=\"./val_text.txt\"):\n",
    "        \"\"\"\n",
    "        create 2 files:\n",
    "        - ./train_text.txt\n",
    "        - ./val_text.txt\n",
    "        \"\"\"\n",
    "        split = int(len(self.text)*(valid_ratio))\n",
    "        with open(train_path, \"w\") as f:\n",
    "            f.write(self.text[split:])\n",
    "        with open(val_path, \"w\") as f:\n",
    "            f.write(self.text[:split])\n",
    "        return train_path, val_path\n",
    "\n",
    "    def show_batch(self, tokenizer, bs:int = 4):\n",
    "        from torch.utils.data.dataloader import DataLoader\n",
    "        bunch = self.create_datasets(tokenizer)\n",
    "        return next(iter(DataLoader(\n",
    "            bunch['train_dataset'],\n",
    "            batch_size=bs,\n",
    "            collate_fn = bunch[\"data_collator\"],\n",
    "            )))\n",
    "\n",
    "    def create_datasets(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        valid_ratio: float = .2,\n",
    "        train_path: str = \"./train_text.txt\",\n",
    "        val_path: str = \"./val_text.txt\",\n",
    "        block_size: int = 128,\n",
    "        mlm: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create pytorch datasets and collating fucntion\n",
    "        - tokenizer: a huggingface tokenizer\n",
    "        - valid ratio: portion of the valid data,\n",
    "            compare to the entire dataset\n",
    "        - train_path: a path saving train text file\n",
    "        - val_path: a path saving valid text file\n",
    "        - block_size: max possible length of the sequence\n",
    "        - mlm, return a masked language modeling collating\n",
    "            default False\n",
    "        \"\"\"\n",
    "        # split dataset\n",
    "        train_path, val_path = self.create_train_val(\n",
    "            valid_ratio=valid_ratio,\n",
    "            train_path=train_path,\n",
    "            val_path=val_path,\n",
    "        )\n",
    "        \n",
    "        from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=train_path,\n",
    "            block_size=block_size)\n",
    "\n",
    "        test_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=val_path,\n",
    "            block_size=block_size)\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=mlm,\n",
    "        )\n",
    "        return dict(\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "    \n",
    "    def get_hf_trainer(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        arguments = None,\n",
    "        valid_ratio: float = .2,\n",
    "        train_path: str = \"./train_text.txt\",\n",
    "        val_path: str = \"./val_text.txt\",\n",
    "        block_size: int = 128,\n",
    "        mlm: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a hugging face trainer\n",
    "        \"\"\"\n",
    "        from transformers import TrainingArguments, Trainer\n",
    "        if arguments is None:\n",
    "            arguments = TrainingArguments(\n",
    "                output_dir=\"./write_style\",\n",
    "                overwrite_output_dir=True,  num_train_epochs=3,\n",
    "                eval_steps = 400, save_steps=800, warmup_steps=600,\n",
    "                per_device_train_batch_size=24,\n",
    "                per_device_eval_batch_size=64,\n",
    "            )\n",
    "        trainer = Trainer(\n",
    "            model=model, args=arguments,\n",
    "            **self.create_datasets(tokenizer,\n",
    "                valid_ratio=valid_ratio,\n",
    "                train_path=train_path,\n",
    "                val_path=val_path,\n",
    "                block_size=block_size,\n",
    "                mlm=mlm)\n",
    "        )\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create interactive uploading widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = Textual.from_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual = uploaded()\n",
    "textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview textual data by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
