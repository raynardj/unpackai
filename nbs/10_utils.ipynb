{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "> Useful function and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from mimetypes import types_map\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core import magic\n",
    "from IPython.display import Javascript\n",
    "from ipywidgets import Output, VBox, Button, HTML\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "PathStr = Union[Path, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python37-32\\lib\\site-packages\\ipytest\\_unittest_support.py:18: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as _pd_testing\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# To be able to run the tests in the Notebook\n",
    "!pip install -q ipytest pytest\n",
    "from pathlib import Path\n",
    "import ipytest\n",
    "import sys\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "root_dir = Path(\"..\").resolve()\n",
    "sys.path.append(str(root_dir / \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "# For Test Cases (might have duplicate import because it will be in a dedicated file)\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from shutil import copy, rmtree\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from PIL import Image\n",
    "from test_common.utils_4_tests import DATA_DIR, IMG_DIR, check_no_log, check_only_warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up error image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we have error image that will interrupt model trainging. We can use ```clean_error_img``` to clean the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_img(\n",
    "    img: Path,\n",
    "    formats: List[str] = [\".jpg\", \".jpeg\", \".png\", \".bmp\"],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Check on a single image,\n",
    "    If it's quality is troublesome\n",
    "        we unlink/ditch the image\n",
    "    \"\"\"\n",
    "    img = Path(img)\n",
    "    # check if this path is an image\n",
    "    if img.suffix.lower().split(\"?\")[0] not in formats:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # try to open that image and then (to check truncated image)\n",
    "        # We need to include in a with block to close the image before deleting\n",
    "        with Image.open(img) as im:\n",
    "            im.load()\n",
    "    except Exception:\n",
    "        if img.exists():\n",
    "            img.unlink()\n",
    "            logging.warning(f\"Removed erroneous img: {img}\")\n",
    "            return\n",
    "\n",
    "\n",
    "def clean_error_img(\n",
    "    path: Path,\n",
    "    progress: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    - path: an image directory\n",
    "    - progress: do we print out progress bar or not\n",
    "        default True\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    # check directory existence\n",
    "    if path.exists() == False:\n",
    "        raise FileExistsError(\n",
    "            f\"\"\"path does not exists on:{path}, \n",
    "    make sure there is a directory \"{path.name}\".\n",
    "    under directory \"{path.parent}\"\n",
    "    \"\"\")\n",
    "\n",
    "    # create iterator, probably with progress bar\n",
    "    iterator = tqdm(list(path.iterdir()), leave=False)\\\n",
    "        if progress else path.iterdir()\n",
    "\n",
    "    for obj in iterator:\n",
    "        if obj.is_dir():\n",
    "            # use recursion to clean the sub folder\n",
    "            clean_error_img(obj, progress=progress)\n",
    "        else:\n",
    "            # cheking on a single image\n",
    "            check_img(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "\n",
    "images_rob = list((IMG_DIR / \"robustness\").glob(\"*.*\"))\n",
    "IMG_RGB = Image.new(\"RGB\", (60, 30), color=(73, 109, 137))\n",
    "IMG_RGBA = Image.new(\"RGBA\", (60, 30), color=(73, 109, 137, 100))\n",
    "\n",
    "@pytest.mark.parametrize(\"img\", images_rob, ids=[i.name for i in images_rob])\n",
    "def test_check_img_error(img: Path, tmpdir, caplog):\n",
    "    \"\"\"Test check_img with incorrect images\"\"\"\n",
    "    img_copy = Path(tmpdir) / img.name\n",
    "    copy(img, img_copy)\n",
    "    check_img(img_copy)\n",
    "    check_only_warning(caplog, img.name)\n",
    "    assert not img_copy.is_file(), f\"File {img_copy} not be deleted\"\n",
    "\n",
    "\n",
    "def test_check_img_empty_wrong_suffix(tmpdir, caplog):\n",
    "    \"\"\"Test check_img with wrong image that does not have a correct extension\"\"\"\n",
    "    img_path = Path(tmpdir) / \"empty.txt\"\n",
    "    img_path.write_bytes(b\"\")\n",
    "    check_img(img_path)\n",
    "    check_no_log(caplog)\n",
    "    assert img_path.is_file(), f\"Image {img_path} not found\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"img\", [IMG_RGB, IMG_RGBA])\n",
    "@pytest.mark.parametrize(\"ext\", [\"png\", \"bmp\", \"jpg\", \"jpeg\"])\n",
    "def test_check_img_correct(img: Image, ext: str, tmpdir, caplog):\n",
    "    \"\"\"Correct that correct image is not removed\"\"\"\n",
    "    alpha_suffix = \"_alpha\" if len(img.getcolors()[0][1]) == 4 else \"\"\n",
    "    if alpha_suffix and ext.startswith(\"jp\"):\n",
    "        pytest.skip(\"JPG does not support RGBA\")\n",
    "\n",
    "    img_path = Path(tmpdir) / f\"correct_image_blue{alpha_suffix}.{ext}\"\n",
    "    img.save(img_path)\n",
    "    check_img(img_path)\n",
    "    check_no_log(caplog)\n",
    "    assert img_path.is_file(), f\"Image {img_path} not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "\n",
    "def test_clean_error_img(tmpdir, monkeypatch) -> None:\n",
    "    \"\"\"Test clean_error_img\"\"\"\n",
    "    for img in images_rob:\n",
    "        copy(img, Path(tmpdir) / img.name)\n",
    "\n",
    "    monkeypatch.chdir(tmpdir)\n",
    "    root = Path(\".\")\n",
    "\n",
    "    sub1 = root / \"sub1\"\n",
    "    sub2 = root / \"sub2\"\n",
    "    sub1.mkdir()\n",
    "    sub2.mkdir()\n",
    "    sub3 = sub2 / \"sub3\"\n",
    "    sub3.mkdir()\n",
    "\n",
    "\n",
    "    (sub1 / \"file11.BMP\").write_text(\"fake image\")\n",
    "    (sub1/\"ðŸ˜±file12 haha.jpg\").write_text(\"fake image\")\n",
    "    (sub1 / \"file13 haha.txt\").write_text(\"not image\")\n",
    "    IMG_RGB.save(sub1 / \"img14_good.jpg\")\n",
    "    IMG_RGBA.save(sub1 / \"img15_good.png\")\n",
    "\n",
    "    (sub2 / \"file21.jpg\").write_text(\"fake image\")\n",
    "    (sub2 / \"file22.jpeg\").write_text(\"fake image\")\n",
    "\n",
    "    (sub3 / \"file31.jpeg\").write_text(\"fake image\")\n",
    "    IMG_RGB.save(sub3 / \"img32_good.jpeg\")\n",
    "    IMG_RGBA.save(sub3 / \"img33_good.bmp\")\n",
    "\n",
    "\n",
    "    good: List[Path] = list()\n",
    "    bad: List[Path] = list()\n",
    "\n",
    "    print(\"Existing files:\")\n",
    "    for file in root.rglob(\"*.*\"):\n",
    "        print(f\" * {file}\")\n",
    "        if file.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "            (good if \"good\" in file.name else bad).append(file)\n",
    "\n",
    "    print(\" => CLEANING\")\n",
    "    clean_error_img(root, progress=False)\n",
    "\n",
    "    good_removed = [f for f in good if not f.is_file()]\n",
    "    assert not good_removed, f\"Good pictures deleted: {good_removed}\"\n",
    "\n",
    "    bad_still_here = [f for f in bad if f.is_file()]\n",
    "    assert not bad_still_here, f\"Bad pictures not deleted: {bad_still_here}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic function ```hush```\n",
    "> Run things quietly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a magical cell function, so remember to use the ```%%```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 versions of hush, the 1st on is the backup one, that using ipywidget as event trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# @register_cell_magic\n",
    "def hush(line, cell):\n",
    "    \"\"\"\n",
    "    A magic cell function to collapse the print out\n",
    "    %%hush\n",
    "    how_loud = 100\n",
    "    how_verbose = \"very\"\n",
    "    do_some_python_thing(how_loud, how_verbose)\n",
    "    \"\"\"\n",
    "    # the current output widget\n",
    "    out = Output()\n",
    "    output_box = VBox([out])\n",
    "    # default setting is to hide the print out\n",
    "    output_box.layout.display = \"none\"\n",
    "    # the toggling button\n",
    "    show_btn = Button(description=\"Show output\")\n",
    "\n",
    "    def toggle_output(o):\n",
    "        # show output, change the button to hide\n",
    "        if output_box.layout.display == \"none\":\n",
    "            output_box.layout.display = \"block\"\n",
    "            show_btn.description = \"Hide output\"\n",
    "        # hide output, change the button to show\n",
    "        else:\n",
    "            output_box.layout.display = \"none\"\n",
    "            show_btn.description = \"Show output\"\n",
    "\n",
    "    # assign toggle to event\n",
    "    show_btn.on_click(toggle_output)\n",
    "\n",
    "    # A control panel containing a button\n",
    "    # and the output box\n",
    "    total_control = VBox([show_btn, output_box])\n",
    "    display(total_control)\n",
    "\n",
    "    with out:\n",
    "        ishell = get_ipython()\n",
    "        # excute the code in cell\n",
    "        result = ishell.run_cell(\n",
    "            cell, silent=False)\n",
    "\n",
    "    # we still want the error to be proclaimed loudly\n",
    "    if result.error_in_exec:\n",
    "        logging.error(f\"'{result.error_in_exec}' happened, breaking silence now\")\n",
    "        result.raise_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this version of ```toggle action``` will be stuck by the ongoing interactive\n",
    "\n",
    "If the process is working on some thing, you can't toggle until the end of the execution.\n",
    "\n",
    "The further improvement will be move the toggle entirely to JavaScript, hence the second version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hush event in JS\n",
    "> As not going through any amount of python backend after run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@magic.register_cell_magic\n",
    "def hush(line, cell):\n",
    "    \"\"\"\n",
    "    A magic cell function to collapse the print out\n",
    "    %%hush\n",
    "    how_loud = 100\n",
    "    how_verbose = \"very\"\n",
    "    do_some_python_thing(how_loud, how_verbose)\n",
    "    \"\"\"\n",
    "    # the current output widget\n",
    "    out = Output()\n",
    "    output_box = VBox([out])\n",
    "    \n",
    "    # create uuid for DOM identifying\n",
    "    uuid = str(uuid4())\n",
    "\n",
    "    # default setting is to hide the print out\n",
    "    output_box.layout.display = \"none\"\n",
    "    # the toggling button\n",
    "    show_btn = Button(description=\"toggle output\")\n",
    "    show_btn.add_class(f\"hush_toggle_{uuid}\")\n",
    "    output_box.add_class(f\"hush_output_{uuid}\")\n",
    "    \n",
    "    \n",
    "    # A control panel containing a button\n",
    "    # and the output box\n",
    "    total_control = VBox([show_btn, output_box])\n",
    "    display(total_control)\n",
    "    \n",
    "    # assign JS listener\n",
    "    display(Javascript(f\"\"\"\n",
    "    console.info(\"loading toggle event: {uuid}\")\n",
    "    const toggle_hush = (e) =>\\u007b\n",
    "        var op = document.querySelector(\".hush_output_{uuid}\");\n",
    "        if(op.style.display==\"none\")\\u007b\n",
    "             op.style.display=\"block\"\n",
    "\n",
    "        \\u007d else \\u007b\n",
    "            op.style.display=\"none\"\n",
    "        \\u007d\n",
    "\n",
    "    \\u007d\n",
    "    document.querySelector('.hush_toggle_{uuid}').onclick=toggle_hush\n",
    "    \"\"\"))\n",
    "\n",
    "    with out:\n",
    "        ishell = get_ipython()\n",
    "        # excute the code in cell\n",
    "        result = ishell.run_cell(\n",
    "            cell, silent=False)\n",
    "\n",
    "    # we still want the error to be proclaimed loudly\n",
    "    if result.error_in_exec:\n",
    "        logging.error(f\"'{result.error_in_exec}' happened, breaking silence now\")\n",
    "        result.raise_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try hushing various kinds of info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html display\n",
    "* print\n",
    "* logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%hush\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.getLogger().setLevel('DEBUG')\n",
    "a = 1\n",
    "for i in range(1000):\n",
    "    print(i, end=\"\\t\")\n",
    "    \n",
    "logging.info(\"a lots of text, VERBOSITY!!\"*100)\n",
    "\n",
    "# a big dataframe\n",
    "display(pd.DataFrame({\"a\":[1,2]*50}))\n",
    "\n",
    "# a big output\n",
    "pd.DataFrame({\"b\":range(100)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capable of open/close verbosity during the main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing hush\n",
    "%%hush\n",
    "from time import sleep\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we still want the error to be loud\n",
    "> As such information should interrupt the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing hush when exception is raised\n",
    "%%hush\n",
    "def _test_hush():\n",
    "    for i in range(20):\n",
    "        print(f\"some logging:\\t{i}!\")\n",
    "    raise ValueError(\"but some error, because life!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching static files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def static_root() -> Path:\n",
    "    \"\"\"\n",
    "    Return static path\n",
    "    \"\"\"\n",
    "    import unpackai\n",
    "\n",
    "    unpackai_path = Path(unpackai.__path__[0])\n",
    "    if not unpackai_path.exists():\n",
    "        egg_path = Path(f\"{unpackai_path}.egg-link\")\n",
    "        unpackai_path = Path(egg_path.read_text())\n",
    "\n",
    "    return unpackai_path / \"static\"\n",
    "\n",
    "\n",
    "STATIC = static_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the validation of such static path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "def test_find_static():\n",
    "    error_report_html = STATIC/\"html\"/\"bug\"/\"error_report.html\"\n",
    "    assert (error_report_html).is_file(), f\"'{STATIC}' is not a valid static path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_url_size(url: str) -> int:\n",
    "    \"\"\"Returns the size of URL, or -1 if it cannot get it\"\"\"\n",
    "    with requests.request(\"HEAD\", url) as resp:\n",
    "        if resp.status_code != 200:\n",
    "            raise ConnectionError(f\"Error when retrieving size from {url}\")\n",
    "\n",
    "        return int(resp.headers.get(\"Content-Length\", -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download(url: str, dest: PathStr = None) -> Path:\n",
    "    \"\"\"Download file and return the Path of the downloaded file\n",
    "\n",
    "    If the destination is not specified, download in the current directory\n",
    "    with the name specified in the URL\n",
    "    \"\"\"\n",
    "    # We can allow empty destination, in which case we use file name in URL\n",
    "    # We need to ensure that destination is a Path\n",
    "    dest = Path(dest or url.rpartition(\"/\")[-1])\n",
    "\n",
    "    # For big files, we will split into chunks\n",
    "    # We might also consider adding a progress bar\n",
    "    size = get_url_size(url)\n",
    "    if size < 1024 * 1024:\n",
    "        with requests.get(url) as resp:\n",
    "            resp.raise_for_status()\n",
    "            dest.write_bytes(resp.content)\n",
    "\n",
    "    else:\n",
    "        with requests.get(url, stream=True) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with dest.open(\"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded {url} to {dest}\")\n",
    "    return dest\n",
    "\n",
    "\n",
    "def url_2_text(url: str) -> str:\n",
    "    \"\"\"Extract text content from an URL (textual content for an HTML)\"\"\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        raise ConnectionError(f\"Error when retrieving text content from {url}\")\n",
    "\n",
    "    resp.encoding = \"utf-8\"\n",
    "    content_type = resp.headers[\"Content-Type\"]\n",
    "    if \"html\" in content_type:\n",
    "        text = BeautifulSoup(resp.text).text\n",
    "    else:\n",
    "        text = resp.text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exportest\n",
    "url_raw_txt = \"https://raw.githubusercontent.com/unpackAI/unpackai/main/test/test_data/to_download.txt\"\n",
    "test_data_txt = (DATA_DIR / \"to_download.txt\").read_text()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def check_connection_github():\n",
    "    try:\n",
    "        with requests.request(\"HEAD\", url_raw_txt, timeout=1) as resp:\n",
    "            resp.raise_for_status()\n",
    "    except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout) as e:\n",
    "        pytest.xfail(f\"Cannot connect to Github: {e}\")\n",
    "\n",
    "\n",
    "def test_get_url_size(check_connection_github):\n",
    "    assert get_url_size(url_raw_txt) == 264, f\"Wrong size for {url_raw_txt}\"\n",
    "\n",
    "\n",
    "def test_download_dest(check_connection_github, tmpdir):\n",
    "    \"\"\"Test download of file to a destination\"\"\"\n",
    "    dest = Path(tmpdir / \"to_download.txt\")\n",
    "    download(url_raw_txt, dest)\n",
    "    assert dest.is_file()\n",
    "    assert dest.read_text() == test_data_txt\n",
    "\n",
    "\n",
    "def test_download_empty(check_connection_github, tmpdir):\n",
    "    \"\"\"Test download of file without destination\"\"\"\n",
    "    dest = Path(\"to_download.txt\")\n",
    "    download(url_raw_txt)\n",
    "    try:\n",
    "        assert dest.is_file()\n",
    "        assert dest.read_text() == test_data_txt\n",
    "    finally:\n",
    "        dest.unlink()\n",
    "\n",
    "\n",
    "def test_url_2_text(check_connection_github):\n",
    "    \"\"\"Test extraction of text from URL\"\"\"\n",
    "    assert url_2_text(url_raw_txt) == test_data_txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing files in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def _iter_files(root: Path, exclude_dir: List[str], hide_info: bool):\n",
    "    \"\"\"Return all info of files found in a root directory\"\"\"\n",
    "    for f in root.rglob(\"*\"):\n",
    "        if any(d in f.parts for d in exclude_dir):\n",
    "            continue\n",
    "\n",
    "        info = {\n",
    "            \"Name\": f.name,\n",
    "            \"Parent\": f.parent.name,\n",
    "            \"Path\": f.as_posix(),\n",
    "            \"Level\": len(f.relative_to(root).parent.parts),\n",
    "            \"Last_Modif\": datetime.fromtimestamp(f.stat().st_mtime),\n",
    "        }\n",
    "        if f.is_file():\n",
    "            info.update(\n",
    "                {\n",
    "                    \"FileDir\": \"File\",\n",
    "                    \"Extension\": f.suffix or f.name,\n",
    "                    \"Type\": types_map.get(f.suffix.lower()),\n",
    "                }\n",
    "            )\n",
    "            if not hide_info:\n",
    "                size = f.stat().st_size\n",
    "                info.update(\n",
    "                    {\n",
    "                        \"Size\": size,\n",
    "                        \"Friendly_Size\": friendly_size(size),\n",
    "                    }\n",
    "                )\n",
    "        elif f.is_dir():\n",
    "            info.update({\"FileDir\": \"Dir\"})\n",
    "\n",
    "        yield info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def friendly_size(size: float) -> str:\n",
    "    \"\"\"Convert a size in bytes (as float) to a size with unit (as a string)\"\"\"\n",
    "    unit = \"B\"\n",
    "    # Reminder: 1 KB = 1024 B, and 1 MB = 1024 KB, ...\n",
    "    for letter in \"KMG\":\n",
    "        if size >= 1024:\n",
    "            size /= 1024\n",
    "            unit = f\"{letter}B\"\n",
    "\n",
    "    # We want to keep 2 digits after floating point\n",
    "    # because it is a good balance between precision and concision\n",
    "    return f\"{size:0.2f} {unit}\"\n",
    "\n",
    "\n",
    "def ls(root: Path, exclude: List[str] = None, hide_info=False) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with list of files & directories in a given path.\n",
    "\n",
    "    Args:\n",
    "        root: root path to look for files and directories recursively\n",
    "        exclude: optional list of names to exclude in the search (e.g. `[\".git\"]`)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        _iter_files(root, exclude_dir=exclude or [], hide_info=hide_info)\n",
    "    )\n",
    "    return df.sort_values(by=[\"Path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "@pytest.mark.parametrize(\n",
    "    \"size,exp\",\n",
    "    [\n",
    "        (0, \"0.00 B\"),\n",
    "        (1, \"1.00 B\"),\n",
    "        (1024, \"1.00 KB\"),\n",
    "        (1024 ** 2, \"1.00 MB\"),\n",
    "        (1024 ** 3 * 3.14, \"3.14 GB\"),\n",
    "    ],\n",
    ")\n",
    "def test_friendly_size(size, exp):\n",
    "    \"\"\"Test computation of friendly size (human readable)\"\"\"\n",
    "    assert friendly_size(size) == exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............                                                            [100%]##vso[results.publish type=JUnit;runTitle='Pytest results';]e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml\n",
      "##vso[task.logissue type=warning;]Coverage XML was not created, skipping upload.\n",
      "\n",
      "-- generated xml file: e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml ---\n",
      "13 passed in 0.14s\n"
     ]
    }
   ],
   "source": [
    "# exportest\n",
    "@pytest.fixture\n",
    "def populated_tmp_dir(tmpdir):\n",
    "    \"\"\"Create files to test `ls` function\"\"\"\n",
    "    tmpdir = Path(tmpdir)\n",
    "    for dir_ in [\"dir1/subdir1\", \"dir1/subdir2\", \"dir2\"]:\n",
    "        (tmpdir / dir_).mkdir(parents=True)\n",
    "    for file in [\"at_root.txt\", \"dir1/subdir1/at_subdir.txt\", \"dir2/at_dir.txt\"]:\n",
    "        (tmpdir / file).write_text(\"unpackai\")\n",
    "    (tmpdir / \"dir1\" / \"subdir2\" / \"some_pic.bmp\").write_bytes(b\"3141590000\")\n",
    "\n",
    "    return tmpdir\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "exp_columns = [\n",
    "    \"Name\",\n",
    "    \"Parent\",\n",
    "    \"Path\",\n",
    "    \"Level\",\n",
    "    \"Last_Modif\",\n",
    "    \"FileDir\",\n",
    "    \"Extension\",\n",
    "    \"Type\",\n",
    "    \"Size\",\n",
    "    \"Friendly_Size\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_ls(populated_tmp_dir):\n",
    "    \"\"\"Test DataFrame generated by `ls` function\"\"\"\n",
    "    df = ls(populated_tmp_dir)\n",
    "    now = datetime.now()\n",
    "    assert list(df.columns) == exp_columns, \"Incorrect columns in DF\"\n",
    "\n",
    "    # We want to copy the list of expecgted columns to keep it intact\n",
    "    columns = exp_columns[:]\n",
    "\n",
    "    assert all(now - timedelta(minutes=5) < date < now for date in df[\"Last_Modif\"])\n",
    "    df.drop(\"Last_Modif\", axis=1, inplace=True)\n",
    "    columns.remove(\"Last_Modif\")\n",
    "\n",
    "    # We check the file \"at_root.txt\" and do some cleanup\n",
    "    at_root = df[df[\"Name\"] == \"at_root.txt\"].iloc[0]\n",
    "    assert Path(at_root[\"Path\"]) == populated_tmp_dir.absolute() / \"at_root.txt\"\n",
    "    assert at_root[\"Friendly_Size\"] == friendly_size(at_root[\"Size\"])\n",
    "    df.drop([\"Path\", \"Friendly_Size\"], axis=1, inplace=True)\n",
    "    columns.remove(\"Path\")\n",
    "    columns.remove(\"Friendly_Size\")\n",
    "\n",
    "    print(\"===TRUNCATED DF FOR LIST OF FILES/DIR===\")\n",
    "    print(df)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    df_exp = pd.DataFrame(\n",
    "        [\n",
    "            (\"at_root.txt\", \"test_ls0\", 0, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "            (\"dir1\", \"test_ls0\", 0, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"subdir1\", \"dir1\", 1, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"at_subdir.txt\", \"subdir1\", 2, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "            (\"subdir2\", \"dir1\", 1, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"some_pic.bmp\", \"subdir2\", 2, \"File\", \".bmp\", \"image/bmp\", 10.0),\n",
    "            (\"dir2\", \"test_ls0\", 0, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"at_dir.txt\", \"dir2\", 1, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "        ],\n",
    "        columns=columns,\n",
    "    )\n",
    "    print(\"===EXPECTED DF FOR LIST OF FILES/DIR===\")\n",
    "    print(df_exp)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    compare_df = df.reset_index(drop=True).compare(df_exp.reset_index(drop=True))\n",
    "    assert compare_df.empty, f\"Differences found when checking DF:\\n{compare_df}\"\n",
    "\n",
    "\n",
    "exp_files = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"subdir1\",\n",
    "    \"at_subdir.txt\",\n",
    "    \"subdir2\",\n",
    "    \"some_pic.bmp\",\n",
    "    \"dir2\",\n",
    "    \"at_dir.txt\",\n",
    "]\n",
    "exp_dir1 = [\"at_root.txt\", \"dir2\", \"at_dir.txt\"]\n",
    "exp_dir2 = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"subdir1\",\n",
    "    \"subdir2\",\n",
    "    \"at_subdir.txt\",\n",
    "    \"some_pic.bmp\",\n",
    "]\n",
    "exp_subdir = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"dir2\",\n",
    "    \"subdir2\",\n",
    "    \"some_pic.bmp\",\n",
    "    \"at_dir.txt\",\n",
    "]\n",
    "exp_dir2_subdir = [\"at_root.txt\", \"dir1\", \"subdir2\", \"some_pic.bmp\"]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"exclude, exp\",\n",
    "    [\n",
    "        ([], exp_files),\n",
    "        ([\"I am not a Dir\"], exp_files),\n",
    "        ([\"dir1\"], exp_dir1),\n",
    "        ([\"dir2\"], exp_dir2),\n",
    "        ([\"subdir1\"], exp_subdir),\n",
    "        ([\"subdir1\", \"dir2\"], exp_dir2_subdir),\n",
    "    ],\n",
    ")\n",
    "def test_ls_exclude(exclude, exp, populated_tmp_dir):\n",
    "    \"\"\"Test `ls` function with an exclusing of some directories\"\"\"\n",
    "    df = ls(populated_tmp_dir, exclude=exclude)\n",
    "    assert sorted(df[\"Name\"]) == sorted(exp)\n",
    "\n",
    "\n",
    "def test_ls_no_info(populated_tmp_dir):\n",
    "    \"\"\"Test `ls` function with `hide_info` set to True\"\"\"\n",
    "    df = ls(populated_tmp_dir, hide_info=True)\n",
    "    assert set(exp_columns) - set(df.columns) == {\"Size\", \"Friendly_Size\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Hide -->\n",
    "# Running all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "try:\n",
    "    ishell = get_ipython()\n",
    "except NameError as e:\n",
    "    from IPython.testing.globalipapp import get_ipython, start_ipython\n",
    "    ishell = start_ipython()\n",
    "    print(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....F                                                                   [100%]##vso[results.publish type=JUnit;runTitle='Pytest results';]e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml\n",
      "##vso[task.logissue type=error;]1 test(s) failed, 6 test(s) collected.\n",
      "##vso[task.logissue type=warning;]Coverage XML was not created, skipping upload.\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________________ test_ls ___________________________________\n",
      "\n",
      "populated_tmp_dir = None\n",
      "\n",
      "    def test_ls(populated_tmp_dir):\n",
      ">       df = ls(populated_tmp_dir)\n",
      "\n",
      "C:\\Users\\jthuong\\AppData\\Local\\Temp/ipykernel_32568/1366368833.py:22: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "C:\\Users\\jthuong\\AppData\\Local\\Temp/ipykernel_32568/3658972182.py:12: in ls\n",
      "    _iter_files(root, exclude_dir=exclude or [], hide_info=hide_info)\n",
      "C:\\Python37-32\\lib\\site-packages\\pandas\\core\\frame.py:563: in __init__\n",
      "    data = list(data)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "root = None, exclude_dir = [], hide_info = False\n",
      "\n",
      "    def _iter_files(root: Path, exclude_dir: List[str], hide_info: bool):\n",
      "        \"\"\"Return all info of files found in a root directory\"\"\"\n",
      ">       for f in root.rglob(\"*.*\"):\n",
      "E       AttributeError: 'NoneType' object has no attribute 'rglob'\n",
      "\n",
      "C:\\Users\\jthuong\\AppData\\Local\\Temp/ipykernel_32568/1243830954.py:18: AttributeError\n",
      "-- generated xml file: e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml ---\n",
      "=========================== short test summary info ===========================\n",
      "FAILED tmph0q8_vxd.py::test_ls - AttributeError: 'NoneType' object has no att...\n",
      "1 failed, 5 passed in 0.27s\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "ipytest.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 32-bit",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
