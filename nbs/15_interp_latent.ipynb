{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "> Easy consine similarity search, search similar features among vectors is a frequently encountered situation\n",
    "\n",
    "> Most of the code is from Ray's other library ```forgebox.cosine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp interp.latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from forgebox.html import DOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CosineSearch:\n",
    "    \"\"\"\n",
    "    Build a index search on cosine distance\n",
    "    cos = CosineSearch(base_array)\n",
    "    idx_order = cos(vec)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base: np.ndarray):\n",
    "        \"\"\"\n",
    "        base: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        \"\"\"\n",
    "        assert len(base.shape) == 2,\\\n",
    "            f\"Base array has to be 2 dimentional, input is {len(base.shape)}\"\n",
    "        self.base = base\n",
    "        self.base_norm = self.calc_base_norm(self.base)\n",
    "        self.normed_base = self.base/self.base_norm[:, None]\n",
    "        self.dim = self.base.shape[1]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"[Consine Similarity Search] ({len(self)} items)\"\n",
    "\n",
    "    def __len__(self): return self.base.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_base_norm(base: np.ndarray) -> np.ndarray:\n",
    "        return np.sqrt(np.power(base, 2).sum(1))\n",
    "\n",
    "    def search(self, vec: np.ndarray, return_similarity: bool = False):\n",
    "        if return_similarity:\n",
    "            similarity = (vec * self.normed_base /\n",
    "                          (np.power(vec, 2).sum())).sum(1)\n",
    "            order = similarity.argsort()[::-1]\n",
    "            return order, similarity[order]\n",
    "        return self(vec)\n",
    "\n",
    "    def __call__(self, vec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the order index of the closest vector to the furthest\n",
    "        vec: an 1 dimentional vector, marks the closest index\n",
    "            to the further ones\n",
    "        \"\"\"\n",
    "        return (vec * self.normed_base).sum(1).argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for similar vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random vector, assimuating ```500 items x 128 embedding hidden size```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.random.rand(500,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Consine Similarity Search] (500 items)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine = CosineSearch(base)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the distance to 6th item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5, 268, 327, 309, 365,  34, 388, 173, 415, 135, 151, 461, 307,\n",
       "       275, 469, 384, 416,  60, 293, 236, 153, 493, 464, 402,  74, 383,\n",
       "        15, 294,  95, 485, 103, 488, 156, 122, 283, 379, 321, 477, 300,\n",
       "       348, 100, 381, 317, 209, 231, 182, 174, 457, 332, 314, 256, 326,\n",
       "       251, 313,   9, 183, 270, 133,  70, 424, 227, 399, 234, 205, 487,\n",
       "        84,  31, 232, 322, 428, 311,  67, 380,  19, 471, 255, 419, 224,\n",
       "       413, 247, 328, 436, 367,  64, 385, 279, 344, 406, 306, 238, 357,\n",
       "       335, 248, 249, 312, 169, 221, 124, 297, 427,  52, 346, 136, 288,\n",
       "       120,  93, 250, 495,  22, 143, 273, 206, 149, 305, 159, 438, 218,\n",
       "       343, 195,  24, 142,  50, 150, 199, 434, 465, 123,  69, 223,  79,\n",
       "       291, 154,  73,  10, 222,  18,  76,  68, 213, 139, 489, 323, 286,\n",
       "       241, 158, 106,  92,  37, 301, 408, 141, 272,   2, 207, 179,  32,\n",
       "       395, 390, 290, 366, 121, 138, 181, 292, 377,  20, 134, 282,   7,\n",
       "       497, 108, 244, 210, 146, 360, 304, 404, 467, 296, 498, 472, 375,\n",
       "        72, 338,  90,  43, 370,  16, 329, 362, 189, 180, 117, 168, 349,\n",
       "        49, 483, 212, 391,  45, 444,  21,  17, 177, 118, 474, 201,  96,\n",
       "       147, 431,  55, 478, 337, 192, 341, 363, 188,  71,  40, 127, 441,\n",
       "       219, 421, 350, 400, 184, 240, 254, 451, 482,  44, 353,  33, 265,\n",
       "       356, 437, 303, 442,  13, 228, 233,  81, 160, 462, 320, 420, 445,\n",
       "        59, 155, 115, 264, 340, 128,  42, 387, 448,  89,  91, 246, 298,\n",
       "       325, 429, 277, 476, 426, 475, 494, 425, 450, 358,  38, 262, 418,\n",
       "       315,  46, 239,  87, 280, 230,  12, 152, 161,  28, 203, 319, 217,\n",
       "        83, 310,  63, 318, 204, 432, 491, 113, 492,   3, 378, 440, 499,\n",
       "       260, 334,  29, 237, 411, 331, 401, 208, 259, 473, 129,  85, 392,\n",
       "       470, 110, 111, 243, 144,  97, 336, 253, 107, 109, 194, 145, 397,\n",
       "       140, 114, 352,   6, 178, 284, 372, 463,  56, 148, 345, 137, 409,\n",
       "       175,  61, 220, 299, 126, 164, 455,  36, 382, 459, 252, 215, 157,\n",
       "       196, 333,   4, 449, 458,  58, 480, 263, 373,  25, 165, 452, 481,\n",
       "       186, 446,  53,  98, 376, 171, 430, 368,  30, 324,  78, 351, 405,\n",
       "       167, 214, 101,  94, 295, 130, 389, 460, 393, 308, 285, 229,  47,\n",
       "       490, 271,  66, 191,  35, 197, 266, 102, 162, 386,   0, 407, 403,\n",
       "       226, 435, 278, 447, 235, 347, 105,   8,  11,  41, 486, 245, 359,\n",
       "       242, 225, 267, 374,  39, 371,  86, 433, 342, 456,  88, 276, 422,\n",
       "       361, 354, 116, 369,  14,  75, 364, 484, 330, 257, 439, 316, 185,\n",
       "       198, 200, 414, 163, 202, 261, 412, 187, 132, 281, 170, 172, 453,\n",
       "        99, 496, 258, 302, 216, 468, 193, 410,  57, 274, 112,  77, 355,\n",
       "       125,  23,   1, 339, 287,  54, 396, 190, 394, 454,  82, 289, 211,\n",
       "        80, 417, 466,  26, 269, 443, 398, 479, 423,  48, 176,  51, 119,\n",
       "        65, 131,  62, 166, 104,  27])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(base[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the distance to 10th item\n",
    "> Returning the similarity value also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.146922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.122393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>436</td>\n",
       "      <td>0.122063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>236</td>\n",
       "      <td>0.121338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>0.120816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>225</td>\n",
       "      <td>0.120507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>113</td>\n",
       "      <td>0.120241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>0.120201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>446</td>\n",
       "      <td>0.119380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>0.119282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order  similarity\n",
       "0      9    0.146922\n",
       "1     28    0.122393\n",
       "2    436    0.122063\n",
       "3    236    0.121338\n",
       "4    135    0.120816\n",
       "5    225    0.120507\n",
       "6    113    0.120241\n",
       "7    160    0.120201\n",
       "8    446    0.119380\n",
       "9     58    0.119282"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order, similarity = cosine.search(base[9], return_similarity=True)\n",
    "\n",
    "pd.DataFrame({\"order\": order, \"similarity\": similarity}).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding interpretation\n",
    "> Interpreting pytorch embedding matrix by utilizing [tensorboard](https://www.tensorflow.org/tensorboard) in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class InterpEmbeddings:\n",
    "    \"\"\"\n",
    "    interp = InterpEmbeddings(embedding_matrix, vocab_dict)\n",
    "    \n",
    "    interp.search(\"computer\")\n",
    "    \n",
    "    # visualize the embedding with tensorboard \n",
    "    interp.visualize_in_tb()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_matrix: np.ndarray,\n",
    "        vocab: Dict[int, str]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        \"\"\"\n",
    "        self.base = embedding_matrix\n",
    "        self.cosine = CosineSearch(embedding_matrix)\n",
    "        self.vocab = vocab\n",
    "        self.c2i = dict((v, k) for k, v in vocab.items())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        cls = self.__class__.__name__\n",
    "        return f\"{cls} with\\n\\t{self.cosine}\"\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        category: str,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and vocabulary dictionary\n",
    "        \"\"\"\n",
    "        token_id = self.c2i.get(category)\n",
    "        if token_id is None:\n",
    "            match_list = []\n",
    "            for token_id, token in self.vocab.items():\n",
    "                if category.lower() in str(token).lower():\n",
    "                    match_list.append({\"token\": token, \"token_id\": token_id})\n",
    "            if len(match_list)==0:\n",
    "                raise KeyError(\n",
    "                    f\"[UnpackAI] category: {category} not in vocabulary\")\n",
    "            else:\n",
    "                match_df = pd.DataFrame(match_list)\n",
    "                DOM(\"Search with the following categories\",\"h3\")()\n",
    "                display(match_df)\n",
    "                token_ids = list(match_df.token_id)\n",
    "        else:\n",
    "            DOM(f\"Search with token id {token_id}\",\"h3\")()\n",
    "            token_ids = [token_id,]\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        \n",
    "        closest = closest[:top_k]\n",
    "        similarity = similarity[:top_k]\n",
    "        tokens = list(self.vocab.get(idx) for idx in closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity})\n",
    "    \n",
    "    def visualize_in_tb(\n",
    "        self,\n",
    "        log_dir:str=\"./logs\",\n",
    "        selection: np.ndarray=None,\n",
    "        first_k:int=500,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the embedding in tensorboard\n",
    "        For now this function is only supported on colab\n",
    "        \"\"\"\n",
    "        # since this won't be excute too many times within a notebook\n",
    "        # in large chances... so to avoid missing library when import\n",
    "        # other function under this module: we import related stuff here\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        # this version's pd has vc for quick value counts\n",
    "        from forgebox.imports import pd\n",
    "        import tensorflow as tf\n",
    "        import tensorboard as tb\n",
    "        import os\n",
    "        \n",
    "        # possible tensorflow version error\n",
    "        tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "        os.system(f\"rm -rf {log_dir}\")\n",
    "        writer = SummaryWriter(log_dir=log_dir,)\n",
    "        self.i2c = dict((v,k) for k,v in self.c2i.items())  \n",
    "        tokens = list(self.i2c.get(i) for i in range(len(self.i2c)))\n",
    "        \n",
    "        if selection is None:\n",
    "            vecs = self.base[:first_k]\n",
    "            tokens = tokens[:first_k]\n",
    "        else:\n",
    "            selection = np.array(selection).astype(dtype=np.int64)\n",
    "            # select a pool of tokens for visualizaiton\n",
    "            tokens = list(np.array(tokens)[selection][:first_k])\n",
    "            vecs = self.base[selection][:first_k]\n",
    "        writer.add_embedding(vecs, metadata=tokens,)\n",
    "        # prompts for next step\n",
    "        print(f\"Please run the the following command in a cell\")\n",
    "        print(\"%load_ext tensorboard\")\n",
    "        print(f\"%tensorboard  --logdir {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage for other task\n",
    "#### eg. recommender sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 500 movies, that we do have learnt latent vectors on these movies\n",
    "\n",
    "Given 1 movie,can we find the most similar ones\n",
    "\n",
    "As we can create feature eg. \"You watched <...>, You may also like...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = 500\n",
    "\n",
    "# an embedding maxtrix, in shape of\n",
    "movie_embedding = np.random.rand(NUM_ITEMS,42)\n",
    "\n",
    "# a dictionary mapping index to string\n",
    "vocab = dict((i, f\"movie #{i}\") for i in range(NUM_ITEMS,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddings(movie_embedding, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Search with token id 22</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie #22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.262194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie #295</td>\n",
       "      <td>295</td>\n",
       "      <td>0.227033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie #108</td>\n",
       "      <td>108</td>\n",
       "      <td>0.225516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie #0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>movie #144</td>\n",
       "      <td>144</td>\n",
       "      <td>0.222527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>movie #232</td>\n",
       "      <td>232</td>\n",
       "      <td>0.222521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>movie #498</td>\n",
       "      <td>498</td>\n",
       "      <td>0.222113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie #381</td>\n",
       "      <td>381</td>\n",
       "      <td>0.220832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie #220</td>\n",
       "      <td>220</td>\n",
       "      <td>0.219798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>movie #305</td>\n",
       "      <td>305</td>\n",
       "      <td>0.218822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>movie #236</td>\n",
       "      <td>236</td>\n",
       "      <td>0.218546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>movie #14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.218388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>movie #274</td>\n",
       "      <td>274</td>\n",
       "      <td>0.217985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie #127</td>\n",
       "      <td>127</td>\n",
       "      <td>0.217889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>movie #326</td>\n",
       "      <td>326</td>\n",
       "      <td>0.217152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>movie #363</td>\n",
       "      <td>363</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie #61</td>\n",
       "      <td>61</td>\n",
       "      <td>0.216979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>movie #352</td>\n",
       "      <td>352</td>\n",
       "      <td>0.216956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>movie #345</td>\n",
       "      <td>345</td>\n",
       "      <td>0.216823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>movie #147</td>\n",
       "      <td>147</td>\n",
       "      <td>0.216784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens  idx  similarity\n",
       "0    movie #22   22    0.262194\n",
       "1   movie #295  295    0.227033\n",
       "2   movie #108  108    0.225516\n",
       "3     movie #0    0    0.223599\n",
       "4   movie #144  144    0.222527\n",
       "5   movie #232  232    0.222521\n",
       "6   movie #498  498    0.222113\n",
       "7   movie #381  381    0.220832\n",
       "8   movie #220  220    0.219798\n",
       "9   movie #305  305    0.218822\n",
       "10  movie #236  236    0.218546\n",
       "11   movie #14   14    0.218388\n",
       "12  movie #274  274    0.217985\n",
       "13  movie #127  127    0.217889\n",
       "14  movie #326  326    0.217152\n",
       "15  movie #363  363    0.217147\n",
       "16   movie #61   61    0.216979\n",
       "17  movie #352  352    0.216956\n",
       "18  movie #345  345    0.216823\n",
       "19  movie #147  147    0.216784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.search(\"movie #22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
