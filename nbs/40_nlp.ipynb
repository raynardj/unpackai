{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "> Things about NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual  Data Obtainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "from ipywidgets import interact, interact_manual, FileUpload\n",
    "from pathlib import Path\n",
    "from forgebox.html import DOM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unpackai.cosine import CosineSearch\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a text data management class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Textual:\n",
    "    \"\"\"\n",
    "    Obtain and manage textual data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"\"\"Text ({len(self.text)} chars), textual(),\n",
    "    train_path, val_path = textual.create_train_val()\"\"\"\n",
    "\n",
    "    def __call__(self, page_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Previewing the first 200(or less) pages\n",
    "        - page_size: character number each page\n",
    "        \"\"\"\n",
    "        logging.info(f\"Previewing the first 200 pages\")\n",
    "\n",
    "        @interact\n",
    "        def show_text(page=(0, min(len(self.text)//page_size-1, 200), 1)):\n",
    "            display(self.text[page*page_size: (page+1)*page_size])\n",
    "\n",
    "    @classmethod\n",
    "    def from_url(cls, url: str):\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 200:\n",
    "            res.encoding = 'utf-8'\n",
    "            headers = res.headers\n",
    "            if \"html\" in str(headers).lower():\n",
    "                text = BeautifulSoup(res.text).text\n",
    "            else:\n",
    "                text = res.text\n",
    "            obj = cls(text)\n",
    "            obj.path = \"./text_data.txt\"\n",
    "            with open(obj.path, \"w\") as f:\n",
    "                f.write(obj.text)\n",
    "            return obj\n",
    "        else:\n",
    "            raise ConnectionError(f\"Error downloading: {url}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: Path):\n",
    "        \"\"\"\n",
    "        Load a textual object from system path\n",
    "        \"\"\"\n",
    "        if Path(path).exists() == False:\n",
    "            raise FileExistsError(f\"Can not find {path}\")\n",
    "        with open(path, ) as f:\n",
    "            obj = cls(f.read())\n",
    "            obj.path = path\n",
    "            return obj\n",
    "    \n",
    "    @classmethod\n",
    "    def from_upload(\n",
    "        cls,\n",
    "        path: Path = Path(\"./uploaded_file.txt\")\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load textual with interactive upload button\n",
    "        \"\"\"\n",
    "        DOM(\"🗃 Please upload a text file ended in .txt\", \"h4\")()\n",
    "        my_manual = interact_manual.options(manual_name=\"Upload\")\n",
    "        @my_manual\n",
    "        def create_upload(btn_upload = FileUpload(description=\"Choose File\")):\n",
    "            text = list(btn_upload.values())[-1]['content'].decode()\n",
    "            with open(path, \"w\") as f:\n",
    "                f.write(text)\n",
    "            return path\n",
    "        def uploaded():\n",
    "            result = create_upload.widget.result\n",
    "            if result is None:\n",
    "                raise FileExistsError(\n",
    "                    \"You have to upload the txt file first\")\n",
    "            return cls.from_path(result)\n",
    "        return uploaded\n",
    "        \n",
    "\n",
    "    def create_train_val(\n",
    "            self,\n",
    "            valid_ratio=.2,\n",
    "            train_path=\"./train_text.txt\",\n",
    "            val_path=\"./val_text.txt\"):\n",
    "        \"\"\"\n",
    "        create 2 files:\n",
    "        - ./train_text.txt\n",
    "        - ./val_text.txt\n",
    "        \"\"\"\n",
    "        split = int(len(self.text)*(valid_ratio))\n",
    "        with open(train_path, \"w\") as f:\n",
    "            f.write(self.text[split:])\n",
    "        with open(val_path, \"w\") as f:\n",
    "            f.write(self.text[:split])\n",
    "        return train_path, val_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>🗃 Please upload a text file ended in .txt</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd61fd3fca8424cbf0784b4b823f88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FileUpload(value={}, description='Choose File'), Button(description='Upload', style=Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploaded = Textual.from_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text (65792 chars), textual(),\n",
       "    train_path, val_path = textual.create_train_val()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual = uploaded()\n",
    "textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b5aea34cfa42258bfeb067d963f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=32, description='page', max=64), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = \"albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f13593270bf4f639e84b7f14256bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80cc4996c948938cff446f48dd368a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d808fdc761364c939caed6b10dd478e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69859ac0e6d483e807782d00dde47ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class InterpEmbeddings:\n",
    "    \"\"\"\n",
    "    interp = InterpEmbeddings(embedding_matrix, vocab_dict)\n",
    "    \n",
    "    interp.search(\"computer\")\n",
    "    \n",
    "    # visualize the embedding with tensorboard \n",
    "    interp.visualize_in_tb()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_matrix: np.ndarray,\n",
    "        vocab: Dict[int, str]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        \"\"\"\n",
    "        self.base = embedding_matrix\n",
    "        self.cosine = CosineSearch(embedding_matrix)\n",
    "        self.vocab = vocab\n",
    "        self.c2i = dict((v, k) for k, v in vocab.items())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        cls = self.__class__.__name__\n",
    "        return f\"{cls} with\\n\\t{self.cosine}\"\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        category: str,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and vocabulary dictionary\n",
    "        \"\"\"\n",
    "        token_id = self.c2i.get(category)\n",
    "        if token_id is None:\n",
    "            match_list = []\n",
    "            for token_id, token in self.vocab.items():\n",
    "                if category.lower() in str(token).lower():\n",
    "                    match_list.append({\"token\": token, \"token_id\": token_id})\n",
    "            if len(match_list)==0:\n",
    "                raise KeyError(\n",
    "                    f\"[UnpackAI] category: {category} not in vocabulary\")\n",
    "            else:\n",
    "                match_df = pd.DataFrame(match_list)\n",
    "                DOM(\"Search with the following categories\",\"h3\")()\n",
    "                display(match_df)\n",
    "                token_ids = list(match_df.token_id)\n",
    "        else:\n",
    "            DOM(f\"Search with token id {token_id}\",\"h3\")()\n",
    "            token_ids = [token_id,]\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        \n",
    "        closest = closest[:top_k]\n",
    "        similarity = similarity[:top_k]\n",
    "        tokens = list(self.vocab.get(idx) for idx in closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity})\n",
    "    \n",
    "    def visualize_in_tb(\n",
    "        self,\n",
    "        log_dir:str=\"./logs\",\n",
    "        selection: np.ndarray=None,\n",
    "        first_k:int=500,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the embedding in tensorboard\n",
    "        For now this function is only supported on colab\n",
    "        \"\"\"\n",
    "        # since this won't be excute too many times within a notebook\n",
    "        # in large chances... so to avoid missing library when import\n",
    "        # other function under this module: we import related stuff here\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        # this version's pd has vc for quick value counts\n",
    "        from forgebox.imports import pd\n",
    "        import tensorflow as tf\n",
    "        import tensorboard as tb\n",
    "        import os\n",
    "        \n",
    "        # possible tensorflow version error\n",
    "        tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "        os.system(f\"rm -rf {log_dir}\")\n",
    "        writer = SummaryWriter(log_dir=log_dir,)\n",
    "        self.i2c = dict((v,k) for k,v in self.c2i.items())  \n",
    "        tokens = list(self.i2c.get(i) for i in range(len(self.i2c)))\n",
    "        \n",
    "        if selection is None:\n",
    "            vecs = self.base[:first_k]\n",
    "            tokens = tokens[:first_k]\n",
    "        else:\n",
    "            # select a pool of tokens for visualizaiton\n",
    "            tokens = tokens[selection][:first_k]\n",
    "            vecs = self.base[selection][:first_k]\n",
    "        writer.add_embedding(vecs, metadata=tokens,)\n",
    "        # prompts for next step\n",
    "        print(f\"Please run the the following command in a cell\")\n",
    "        print(\"%load_ext tensorboard\")\n",
    "        print(f\"%tensorboard  --logdir {log_dir}\")\n",
    "\n",
    "\n",
    "class InterpEmbeddingsTokenizer(InterpEmbeddings):\n",
    "    def __init__(self,\n",
    "                 embedding_matrix,\n",
    "                 tokenizer):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        tokenizer: a huggingface tokenizer\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            embedding_matrix,\n",
    "            dict((v, k) for k, v in tokenizer.vocab.items()))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        word: str,\n",
    "        filter_special_token: bool = True,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and\n",
    "            tokenizer's encode/ decode\n",
    "        \"\"\"\n",
    "        token_ids = self.tokenizer.encode(word)\n",
    "        if filter_special_token:\n",
    "            token_ids = list(t for t in token_ids if t > 110)\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity}).head(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[CLS]',\n",
       " 'eos_token': '[SEP]',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertEmbeddings(\n",
       "  (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 128)\n",
       "  (token_type_embeddings): Embedding(2, 128)\n",
       "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 128)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = model.embeddings.word_embeddings.weight.data.numpy()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with tokenizer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddingsTokenizer(\n",
    "    embedding_matrix,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁wife</td>\n",
       "      <td>663</td>\n",
       "      <td>2.796573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁girlfriend</td>\n",
       "      <td>5606</td>\n",
       "      <td>1.874535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁spouse</td>\n",
       "      <td>16663</td>\n",
       "      <td>1.867864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁wives</td>\n",
       "      <td>11333</td>\n",
       "      <td>1.867352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁husband</td>\n",
       "      <td>1253</td>\n",
       "      <td>1.826611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>▁daughter</td>\n",
       "      <td>783</td>\n",
       "      <td>1.810506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>wife</td>\n",
       "      <td>13611</td>\n",
       "      <td>1.751927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>▁widow</td>\n",
       "      <td>5151</td>\n",
       "      <td>1.605985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>▁fiancee</td>\n",
       "      <td>22947</td>\n",
       "      <td>1.557384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>▁sister</td>\n",
       "      <td>1035</td>\n",
       "      <td>1.515947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>▁daughters</td>\n",
       "      <td>4909</td>\n",
       "      <td>1.495724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>▁woman</td>\n",
       "      <td>524</td>\n",
       "      <td>1.459538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>▁son</td>\n",
       "      <td>433</td>\n",
       "      <td>1.451740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>▁married</td>\n",
       "      <td>567</td>\n",
       "      <td>1.424582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>▁fiance</td>\n",
       "      <td>22324</td>\n",
       "      <td>1.421876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>▁granddaughter</td>\n",
       "      <td>15176</td>\n",
       "      <td>1.416612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>▁bride</td>\n",
       "      <td>8034</td>\n",
       "      <td>1.412241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>▁mother</td>\n",
       "      <td>449</td>\n",
       "      <td>1.410529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>▁mistress</td>\n",
       "      <td>10427</td>\n",
       "      <td>1.394689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>▁partner</td>\n",
       "      <td>2417</td>\n",
       "      <td>1.379885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tokens    idx  similarity\n",
       "0            ▁wife    663    2.796573\n",
       "1      ▁girlfriend   5606    1.874535\n",
       "2          ▁spouse  16663    1.867864\n",
       "3           ▁wives  11333    1.867352\n",
       "4         ▁husband   1253    1.826611\n",
       "5        ▁daughter    783    1.810506\n",
       "6             wife  13611    1.751927\n",
       "7           ▁widow   5151    1.605985\n",
       "8         ▁fiancee  22947    1.557384\n",
       "9          ▁sister   1035    1.515947\n",
       "10      ▁daughters   4909    1.495724\n",
       "11          ▁woman    524    1.459538\n",
       "12            ▁son    433    1.451740\n",
       "13        ▁married    567    1.424582\n",
       "14         ▁fiance  22324    1.421876\n",
       "15  ▁granddaughter  15176    1.416612\n",
       "16          ▁bride   8034    1.412241\n",
       "17         ▁mother    449    1.410529\n",
       "18       ▁mistress  10427    1.394689\n",
       "19        ▁partner   2417    1.379885"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.search(\"wife\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with just an token id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = 500\n",
    "\n",
    "# an embedding maxtrix, in shape of\n",
    "movie_embedding = np.random.rand(NUM_ITEMS,42)\n",
    "\n",
    "# a dictionary mapping index to string\n",
    "vocab = dict((i, f\"movie #{i}\") for i in range(NUM_ITEMS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the example embedding and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddings(movie_embedding, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Search with token id 22</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>movie #22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.287944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>movie #74</td>\n",
       "      <td>74</td>\n",
       "      <td>0.240893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>movie #346</td>\n",
       "      <td>346</td>\n",
       "      <td>0.238294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>movie #239</td>\n",
       "      <td>239</td>\n",
       "      <td>0.238032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>movie #126</td>\n",
       "      <td>126</td>\n",
       "      <td>0.236948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>movie #414</td>\n",
       "      <td>414</td>\n",
       "      <td>0.236702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>movie #457</td>\n",
       "      <td>457</td>\n",
       "      <td>0.235410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>movie #458</td>\n",
       "      <td>458</td>\n",
       "      <td>0.234861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>movie #256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.233834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>movie #138</td>\n",
       "      <td>138</td>\n",
       "      <td>0.233646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>movie #376</td>\n",
       "      <td>376</td>\n",
       "      <td>0.233056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>movie #311</td>\n",
       "      <td>311</td>\n",
       "      <td>0.231898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>movie #321</td>\n",
       "      <td>321</td>\n",
       "      <td>0.231871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>movie #103</td>\n",
       "      <td>103</td>\n",
       "      <td>0.231282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>movie #323</td>\n",
       "      <td>323</td>\n",
       "      <td>0.231186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>movie #198</td>\n",
       "      <td>198</td>\n",
       "      <td>0.231019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>movie #258</td>\n",
       "      <td>258</td>\n",
       "      <td>0.230791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>movie #408</td>\n",
       "      <td>408</td>\n",
       "      <td>0.230484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>movie #464</td>\n",
       "      <td>464</td>\n",
       "      <td>0.230410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>movie #159</td>\n",
       "      <td>159</td>\n",
       "      <td>0.230120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens  idx  similarity\n",
       "0    movie #22   22    0.287944\n",
       "1    movie #74   74    0.240893\n",
       "2   movie #346  346    0.238294\n",
       "3   movie #239  239    0.238032\n",
       "4   movie #126  126    0.236948\n",
       "5   movie #414  414    0.236702\n",
       "6   movie #457  457    0.235410\n",
       "7   movie #458  458    0.234861\n",
       "8   movie #256  256    0.233834\n",
       "9   movie #138  138    0.233646\n",
       "10  movie #376  376    0.233056\n",
       "11  movie #311  311    0.231898\n",
       "12  movie #321  321    0.231871\n",
       "13  movie #103  103    0.231282\n",
       "14  movie #323  323    0.231186\n",
       "15  movie #198  198    0.231019\n",
       "16  movie #258  258    0.230791\n",
       "17  movie #408  408    0.230484\n",
       "18  movie #464  464    0.230410\n",
       "19  movie #159  159    0.230120"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.search(\"movie #22\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
